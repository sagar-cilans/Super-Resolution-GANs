{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SRGAN_PyTorch.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "BEg5pW-QXHXx"
      },
      "source": [
        "! pip install -q kaggle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sEBrfGBMB1R0"
      },
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D4C3QkZ6CGPP"
      },
      "source": [
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LeIaZlz6CLJB"
      },
      "source": [
        "! kaggle datasets download -d akhileshdkapse/super-image-resolution"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IjcJ70FgCXTK"
      },
      "source": [
        "! unzip super-image-resolution.zip -d /content/drive/MyDrive/ColabNotebooks/GANs/SRGAN/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwTKlJFGtQM4"
      },
      "source": [
        "# Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jFQCUT6KtPMo",
        "outputId": "a9afa75b-3be6-4fc4-bfa5-5dd3f0395021"
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import transforms\n",
        "import numpy as np\n",
        "import math\n",
        "from torchvision.models.vgg import vgg16\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "from torchvision.utils import save_image\n",
        "from torch.autograd import Variable\n",
        "\n",
        "torch.autograd.set_detect_anomaly(True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x7f9a68e5bc10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "up03xkzKtvl_"
      },
      "source": [
        "# Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mp7DPA8lCsWg"
      },
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, channels):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(channels, channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(channels),\n",
        "            nn.PReLU(),\n",
        "            nn.Conv2d(channels, channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(channels)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "class UpsampleBlock(nn.Module):\n",
        "    def __init__(self, in_channels, up_scale):\n",
        "        super(UpsampleBlock, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, in_channels*up_scale**2, kernel_size=3, padding=1),\n",
        "            nn.PixelShuffle(up_scale),\n",
        "            nn.PReLU()\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, scale_factor):\n",
        "        upsample_block_num = int(math.log(scale_factor, 2))\n",
        "        super(Generator, self).__init__()\n",
        "        self.block1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=9, padding=4),\n",
        "            nn.PReLU()\n",
        "        )\n",
        "        self.block2 = ResidualBlock(64)\n",
        "        self.block3 = ResidualBlock(64)\n",
        "        self.block4 = ResidualBlock(64)\n",
        "        self.block5 = ResidualBlock(64)\n",
        "        self.block6 = ResidualBlock(64)\n",
        "        self.block7 = nn.Sequential(\n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64)\n",
        "        )\n",
        "        block8 = [UpsampleBlock(64, 2) for _ in range(upsample_block_num)]\n",
        "        block8.append(nn.Conv2d(64, 3, kernel_size=9, padding=4))\n",
        "        self.block8 = nn.Sequential(*block8)\n",
        "\n",
        "    def forward(self, x):\n",
        "        block1 = self.block1(x)\n",
        "        block2 = self.block2(block1)\n",
        "        block3 = self.block3(block2)\n",
        "        block4 = self.block4(block3)\n",
        "        block5 = self.block5(block4)\n",
        "        block6 = self.block6(block5)\n",
        "        block7 = self.block7(block6)\n",
        "        block8 = self.block8(block1 + block7)\n",
        "\n",
        "        return (torch.tanh(block8) + 1) / 2   "
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JOGxWG9jt5rd"
      },
      "source": [
        "# Discriminator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7weTcott1gA"
      },
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
        "            nn.LeakyReLU(0.2),\n",
        "\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.LeakyReLU(0.2),\n",
        "\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(0.2),\n",
        "\n",
        "            nn.Conv2d(128, 128, kernel_size=3, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(0.2),\n",
        "\n",
        "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.LeakyReLU(0.2),\n",
        "\n",
        "            nn.Conv2d(256, 256, kernel_size=3, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.LeakyReLU(0.2),\n",
        "\n",
        "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.LeakyReLU(0.2),\n",
        "\n",
        "            nn.Conv2d(512, 512, kernel_size=3, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.LeakyReLU(0.2),\n",
        "\n",
        "            nn.AdaptiveAvgPool2d(1),\n",
        "            nn.Conv2d(512, 1024, kernel_size=1),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Conv2d(1024, 1, kernel_size=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.size(0)\n",
        "        return torch.sigmoid(self.net(x).view(batch_size))"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJbjfJnRuCHl"
      },
      "source": [
        "# Loss FUnctions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-wwMUKCt-Ug"
      },
      "source": [
        "class GeneratorLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GeneratorLoss, self).__init__()\n",
        "        vgg = vgg16(pretrained=True)\n",
        "        loss_network = nn.Sequential(*list(vgg.features)[:31]).eval()\n",
        "        for param in loss_network.parameters():\n",
        "            param.requires_grad = False\n",
        "        self.loss_network = loss_network\n",
        "        self.mse_loss = nn.MSELoss()\n",
        "        self.tv_loss = TVLoss()\n",
        "\n",
        "    def forward(self, out_labels, out_images, target_images):\n",
        "        # Adversarial Loss\n",
        "        adversarial_loss = torch.mean(1 - out_labels)\n",
        "        # Perception Loss\n",
        "        perception_loss = self.mse_loss(self.loss_network(out_images), self.loss_network(target_images))\n",
        "        # Image Loss\n",
        "        image_loss = self.mse_loss(out_images, target_images)\n",
        "        # TV Loss\n",
        "        tv_loss = self.tv_loss(out_images)\n",
        "        return image_loss + 0.001 * adversarial_loss + 0.006 * perception_loss + 2e-8 * tv_loss\n",
        "\n",
        "class TVLoss(nn.Module):\n",
        "    def __init__(self, tv_loss_weight=1):\n",
        "        super(TVLoss, self).__init__()\n",
        "        self.tv_loss_weight = tv_loss_weight\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.size()[0]\n",
        "        h_x = x.size()[2]\n",
        "        w_x = x.size()[3]\n",
        "        count_h = self.tensor_size(x[:, :, 1:, :])\n",
        "        count_w = self.tensor_size(x[:, :, :, 1:])\n",
        "        h_tv = torch.pow((x[:, :, 1:, :] - x[:, :, :h_x - 1, :]), 2).sum()\n",
        "        w_tv = torch.pow((x[:, :, :, 1:] - x[:, :, :, :w_x - 1]), 2).sum()\n",
        "        return self.tv_loss_weight * 2 * (h_tv / count_h + w_tv / count_w) / batch_size\n",
        "\n",
        "    @staticmethod\n",
        "    def tensor_size(t):\n",
        "        return t.size()[1] * t.size()[2] * t.size()[3]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ll838aUCuHku"
      },
      "source": [
        "# Config"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3GZiHXvuECw"
      },
      "source": [
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "INPUT_DIR = '/content/drive/MyDrive/ColabNotebooks/GANs/SRGAN/Data/LR'\n",
        "TARGET_DIR = '/content/drive/MyDrive/ColabNotebooks/GANs/SRGAN/Data/HR'\n",
        "# INPUT_DIR_TEST = '../data/LR_test/'\n",
        "# TARGET_DIR_TEST = '../data/HR_test/'\n",
        "UPSCALE_FACTOR = 4\n",
        "CROP_SIZE = 88\n",
        "mean = np.array([0.485, 0.456, 0.406])\n",
        "std = np.array([0.229, 0.224, 0.225])\n",
        "LEARNING_RATE = 0.0002\n",
        "BATCH_SIZE = 16\n",
        "NUM_WORKERS = 2\n",
        "NUM_EPOCHS = 150\n",
        "LOAD_MODEL = False\n",
        "SAVE_MODEL = False\n",
        "CHECKPOINT_DISC = \"disc.pth.tar\"\n",
        "CHECKPOINT_GEN = \"gen.pth.tar\"\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.RandomVerticalFlip(),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "])"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_X71ZNozuW4V"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxmKHjNkuJWr"
      },
      "source": [
        "class MapDataset(Dataset):\n",
        "    def __init__(self, input_dir, target_dir):\n",
        "        self.input_dir = input_dir\n",
        "        self.target_dir = target_dir\n",
        "        self.input_files = os.listdir(self.input_dir)\n",
        "        self.target_files = os.listdir(self.target_dir)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_files)\n",
        "\n",
        "    def __getitem__(self, index=0):\n",
        "        input_img_file = self.input_files[index]\n",
        "        input_img_path = os.path.join(self.input_dir, input_img_file)\n",
        "        input_image = Image.open(input_img_path)\n",
        "        input_image = input_image.resize((64 , 64))\n",
        "        input_image = transform(input_image)\n",
        "\n",
        "        target_img_file = self.target_files[index]\n",
        "        target_img_path = os.path.join(self.target_dir, target_img_file)\n",
        "        target_image = Image.open(target_img_path)\n",
        "        target_image = target_image.resize((256 , 256))\n",
        "        target_image = transform(target_image)\n",
        "\n",
        "        return input_image, target_image\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBGOm2dQuw3I"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FqMyWfI9ubIa",
        "outputId": "e6fc5b71-02b5-4cea-b8c0-4738c7c99af4"
      },
      "source": [
        "def train_fn(disc, gen, loader, generator_criterion, opt_disc, opt_gen, epoch):\n",
        "    train_bar = tqdm(loader)\n",
        "    running_results = {'batch_sizes': 0, 'd_loss': 0, 'g_loss': 0, 'd_score': 0, 'g_score': 0}        \n",
        "    \n",
        "    gen.train()\n",
        "    disc.train()\n",
        "\n",
        "    for data, target in train_bar:\n",
        "        g_update_first = True\n",
        "        batch_size = data.size(0)\n",
        "        running_results['batch_sizes'] += batch_size\n",
        "\n",
        "        real_img = Variable(target)\n",
        "        if torch.cuda.is_available():\n",
        "            real_img = real_img.cuda()\n",
        "        z = Variable(data)\n",
        "        if torch.cuda.is_available():\n",
        "            z = z.cuda()\n",
        "\n",
        "        # Update Discriminator Network\n",
        "        fake_img = gen(z)\n",
        "\n",
        "        disc.zero_grad()\n",
        "        real_out = disc(real_img).mean()\n",
        "        fake_out = disc(fake_img).mean()\n",
        "        d_loss = 1 - real_out + fake_out\n",
        "        d_loss.backward(retain_graph=True)\n",
        "        opt_disc.step()\n",
        "\n",
        "         # Update Generator network\n",
        "        fake_img = gen(z)\n",
        "        fake_out = disc(fake_img).mean()\n",
        "\n",
        "        gen.zero_grad()\n",
        "        g_loss = generator_criterion(fake_out, fake_img, real_img)\n",
        "        g_loss.backward()\n",
        "\n",
        "        fake_img = gen(z)\n",
        "        fake_out = disc(fake_img).mean()\n",
        "\n",
        "        opt_gen.step()\n",
        "\n",
        "        running_results['g_loss'] += g_loss.item() * batch_size\n",
        "        running_results['d_loss'] += d_loss.item() * batch_size\n",
        "        running_results['d_score'] += real_out.item() * batch_size\n",
        "        running_results['g_score'] += fake_out.item() * batch_size\n",
        "\n",
        "        train_bar.set_description(desc='[%d/%d] Loss_D: %.4f Loss_G: %.4f D(x): %.4f D(G(z)): %.4f' % (\n",
        "            epoch, NUM_EPOCHS, running_results['d_loss'] / running_results['batch_sizes'],\n",
        "            running_results['g_loss'] / running_results['batch_sizes'],\n",
        "            running_results['d_score'] / running_results['batch_sizes'],\n",
        "            running_results['g_score'] / running_results['batch_sizes']))\n",
        "\n",
        "    gen.eval()\n",
        "    out_path = 'training_results/SRF_' + str(UPSCALE_FACTOR) + '/'\n",
        "    if not os.path.exists(out_path):\n",
        "        os.makedirs(out_path)\n",
        "    torch.save(gen.state_dict(), \"super_res_gen.pth\")\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "    disc = Discriminator().to(DEVICE)\n",
        "    gen = Generator(UPSCALE_FACTOR).to(DEVICE)\n",
        "    generator_criterion = GeneratorLoss().to(DEVICE)\n",
        "\n",
        "    opt_disc = optim.Adam(disc.parameters(), lr=LEARNING_RATE)\n",
        "    opt_gen = optim.Adam(gen.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "    results = {'d_loss': [], 'g_loss': [], 'd_score': [], 'g_score': [], 'psnr': [], 'ssim': []}\n",
        "\n",
        "    train_dataset = MapDataset(INPUT_DIR, TARGET_DIR)\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=False,\n",
        "        num_workers=NUM_WORKERS,\n",
        "    )\n",
        "    \n",
        "\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        train_fn(disc, gen, train_loader, generator_criterion, opt_disc, opt_gen, epoch)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0/150] Loss_D: 0.9101 Loss_G: 0.0813 D(x): 0.5637 D(G(z)): 0.4423: 100%|██████████| 7/7 [00:17<00:00,  2.48s/it]\n",
            "[1/150] Loss_D: 0.6657 Loss_G: 0.0814 D(x): 0.6413 D(G(z)): 0.2791: 100%|██████████| 7/7 [00:17<00:00,  2.50s/it]\n",
            "[2/150] Loss_D: 0.4163 Loss_G: 0.0810 D(x): 0.7866 D(G(z)): 0.1856: 100%|██████████| 7/7 [00:17<00:00,  2.52s/it]\n",
            "[3/150] Loss_D: 0.2203 Loss_G: 0.0790 D(x): 0.8864 D(G(z)): 0.0933: 100%|██████████| 7/7 [00:17<00:00,  2.55s/it]\n",
            "[4/150] Loss_D: 0.1073 Loss_G: 0.0792 D(x): 0.9414 D(G(z)): 0.0432: 100%|██████████| 7/7 [00:18<00:00,  2.58s/it]\n",
            "[5/150] Loss_D: 0.0531 Loss_G: 0.0785 D(x): 0.9717 D(G(z)): 0.0224: 100%|██████████| 7/7 [00:18<00:00,  2.59s/it]\n",
            "[6/150] Loss_D: 0.0246 Loss_G: 0.0779 D(x): 0.9881 D(G(z)): 0.0115: 100%|██████████| 7/7 [00:18<00:00,  2.62s/it]\n",
            "[7/150] Loss_D: 0.0129 Loss_G: 0.0774 D(x): 0.9940 D(G(z)): 0.0063: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[8/150] Loss_D: 0.0080 Loss_G: 0.0778 D(x): 0.9962 D(G(z)): 0.0038: 100%|██████████| 7/7 [00:18<00:00,  2.64s/it]\n",
            "[9/150] Loss_D: 0.0054 Loss_G: 0.0773 D(x): 0.9972 D(G(z)): 0.0024: 100%|██████████| 7/7 [00:18<00:00,  2.64s/it]\n",
            "[10/150] Loss_D: 0.0039 Loss_G: 0.0770 D(x): 0.9979 D(G(z)): 0.0017: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[11/150] Loss_D: 0.0032 Loss_G: 0.0778 D(x): 0.9983 D(G(z)): 0.0014: 100%|██████████| 7/7 [00:18<00:00,  2.64s/it]\n",
            "[12/150] Loss_D: 0.0026 Loss_G: 0.0771 D(x): 0.9986 D(G(z)): 0.0012: 100%|██████████| 7/7 [00:18<00:00,  2.64s/it]\n",
            "[13/150] Loss_D: 0.0022 Loss_G: 0.0763 D(x): 0.9988 D(G(z)): 0.0010: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[14/150] Loss_D: 0.0020 Loss_G: 0.0758 D(x): 0.9990 D(G(z)): 0.0009: 100%|██████████| 7/7 [00:18<00:00,  2.64s/it]\n",
            "[15/150] Loss_D: 0.0018 Loss_G: 0.0759 D(x): 0.9990 D(G(z)): 0.0009: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[16/150] Loss_D: 0.0016 Loss_G: 0.0762 D(x): 0.9992 D(G(z)): 0.0007: 100%|██████████| 7/7 [00:18<00:00,  2.64s/it]\n",
            "[17/150] Loss_D: 0.0015 Loss_G: 0.0755 D(x): 0.9992 D(G(z)): 0.0007: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[18/150] Loss_D: 0.0014 Loss_G: 0.0751 D(x): 0.9993 D(G(z)): 0.0007: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[19/150] Loss_D: 0.0012 Loss_G: 0.0749 D(x): 0.9993 D(G(z)): 0.0005: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[20/150] Loss_D: 0.0012 Loss_G: 0.0765 D(x): 0.9994 D(G(z)): 0.0005: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[21/150] Loss_D: 0.0011 Loss_G: 0.0764 D(x): 0.9994 D(G(z)): 0.0005: 100%|██████████| 7/7 [00:18<00:00,  2.64s/it]\n",
            "[22/150] Loss_D: 0.0010 Loss_G: 0.0758 D(x): 0.9995 D(G(z)): 0.0004: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[23/150] Loss_D: 0.0011 Loss_G: 0.0761 D(x): 0.9995 D(G(z)): 0.0005: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[24/150] Loss_D: 0.0009 Loss_G: 0.0758 D(x): 0.9995 D(G(z)): 0.0004: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[25/150] Loss_D: 0.0008 Loss_G: 0.0748 D(x): 0.9996 D(G(z)): 0.0003: 100%|██████████| 7/7 [00:18<00:00,  2.64s/it]\n",
            "[26/150] Loss_D: 0.0007 Loss_G: 0.0751 D(x): 0.9996 D(G(z)): 0.0003: 100%|██████████| 7/7 [00:18<00:00,  2.64s/it]\n",
            "[27/150] Loss_D: 0.0007 Loss_G: 0.0730 D(x): 0.9996 D(G(z)): 0.0003: 100%|██████████| 7/7 [00:18<00:00,  2.64s/it]\n",
            "[28/150] Loss_D: 0.0007 Loss_G: 0.0747 D(x): 0.9996 D(G(z)): 0.0003: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[29/150] Loss_D: 0.0006 Loss_G: 0.0743 D(x): 0.9997 D(G(z)): 0.0003: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[30/150] Loss_D: 0.0006 Loss_G: 0.0749 D(x): 0.9997 D(G(z)): 0.0002: 100%|██████████| 7/7 [00:18<00:00,  2.64s/it]\n",
            "[31/150] Loss_D: 0.0005 Loss_G: 0.0741 D(x): 0.9997 D(G(z)): 0.0002: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[32/150] Loss_D: 0.0005 Loss_G: 0.0738 D(x): 0.9997 D(G(z)): 0.0002: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[33/150] Loss_D: 0.0005 Loss_G: 0.0729 D(x): 0.9997 D(G(z)): 0.0002: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[34/150] Loss_D: 0.0004 Loss_G: 0.0746 D(x): 0.9998 D(G(z)): 0.0002: 100%|██████████| 7/7 [00:18<00:00,  2.64s/it]\n",
            "[35/150] Loss_D: 0.0004 Loss_G: 0.0730 D(x): 0.9998 D(G(z)): 0.0002: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[36/150] Loss_D: 0.0004 Loss_G: 0.0740 D(x): 0.9998 D(G(z)): 0.0002: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[37/150] Loss_D: 0.0004 Loss_G: 0.0735 D(x): 0.9998 D(G(z)): 0.0001: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[38/150] Loss_D: 0.0004 Loss_G: 0.0727 D(x): 0.9998 D(G(z)): 0.0002: 100%|██████████| 7/7 [00:18<00:00,  2.64s/it]\n",
            "[39/150] Loss_D: 0.0004 Loss_G: 0.0727 D(x): 0.9998 D(G(z)): 0.0002: 100%|██████████| 7/7 [00:18<00:00,  2.64s/it]\n",
            "[40/150] Loss_D: 0.0003 Loss_G: 0.0722 D(x): 0.9998 D(G(z)): 0.0001: 100%|██████████| 7/7 [00:18<00:00,  2.64s/it]\n",
            "[41/150] Loss_D: 0.0003 Loss_G: 0.0714 D(x): 0.9998 D(G(z)): 0.0001: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[42/150] Loss_D: 0.0003 Loss_G: 0.0706 D(x): 0.9998 D(G(z)): 0.0001: 100%|██████████| 7/7 [00:18<00:00,  2.64s/it]\n",
            "[43/150] Loss_D: 0.0003 Loss_G: 0.0719 D(x): 0.9998 D(G(z)): 0.0001: 100%|██████████| 7/7 [00:18<00:00,  2.64s/it]\n",
            "[44/150] Loss_D: 0.0003 Loss_G: 0.0712 D(x): 0.9999 D(G(z)): 0.0001: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[45/150] Loss_D: 0.0003 Loss_G: 0.0725 D(x): 0.9999 D(G(z)): 0.0001: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[46/150] Loss_D: 0.0002 Loss_G: 0.0722 D(x): 0.9999 D(G(z)): 0.0001: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[47/150] Loss_D: 0.0002 Loss_G: 0.0704 D(x): 0.9999 D(G(z)): 0.0001: 100%|██████████| 7/7 [00:18<00:00,  2.64s/it]\n",
            "[48/150] Loss_D: 0.0002 Loss_G: 0.0711 D(x): 0.9999 D(G(z)): 0.0001: 100%|██████████| 7/7 [00:18<00:00,  2.64s/it]\n",
            "[49/150] Loss_D: 0.0002 Loss_G: 0.0713 D(x): 0.9999 D(G(z)): 0.0001: 100%|██████████| 7/7 [00:18<00:00,  2.64s/it]\n",
            "[50/150] Loss_D: 0.0002 Loss_G: 0.0719 D(x): 0.9999 D(G(z)): 0.0001: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[51/150] Loss_D: 0.0002 Loss_G: 0.0709 D(x): 0.9999 D(G(z)): 0.0001: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[52/150] Loss_D: 0.0002 Loss_G: 0.0715 D(x): 0.9999 D(G(z)): 0.0001: 100%|██████████| 7/7 [00:18<00:00,  2.64s/it]\n",
            "[53/150] Loss_D: 0.0002 Loss_G: 0.0705 D(x): 0.9999 D(G(z)): 0.0001: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[54/150] Loss_D: 0.0002 Loss_G: 0.0706 D(x): 0.9999 D(G(z)): 0.0001: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[55/150] Loss_D: 0.0002 Loss_G: 0.0707 D(x): 0.9999 D(G(z)): 0.0001: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[56/150] Loss_D: 0.0002 Loss_G: 0.0700 D(x): 0.9999 D(G(z)): 0.0001: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[57/150] Loss_D: 0.0002 Loss_G: 0.0701 D(x): 0.9999 D(G(z)): 0.0001: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[58/150] Loss_D: 0.0002 Loss_G: 0.0690 D(x): 0.9999 D(G(z)): 0.0001: 100%|██████████| 7/7 [00:18<00:00,  2.64s/it]\n",
            "[59/150] Loss_D: 0.0002 Loss_G: 0.0699 D(x): 0.9999 D(G(z)): 0.0001: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[60/150] Loss_D: 0.0002 Loss_G: 0.0686 D(x): 0.9999 D(G(z)): 0.0001: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[61/150] Loss_D: 0.0001 Loss_G: 0.0692 D(x): 0.9999 D(G(z)): 0.0001: 100%|██████████| 7/7 [00:18<00:00,  2.64s/it]\n",
            "[62/150] Loss_D: 0.0001 Loss_G: 0.0690 D(x): 0.9999 D(G(z)): 0.0001: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[63/150] Loss_D: 0.0001 Loss_G: 0.0702 D(x): 0.9999 D(G(z)): 0.0001: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[64/150] Loss_D: 0.0001 Loss_G: 0.0692 D(x): 0.9999 D(G(z)): 0.0001: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[65/150] Loss_D: 0.0001 Loss_G: 0.0693 D(x): 0.9999 D(G(z)): 0.0001: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[66/150] Loss_D: 0.0001 Loss_G: 0.0695 D(x): 0.9999 D(G(z)): 0.0001: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[67/150] Loss_D: 0.0002 Loss_G: 0.0700 D(x): 0.9999 D(G(z)): 0.0005: 100%|██████████| 7/7 [00:18<00:00,  2.62s/it]\n",
            "[68/150] Loss_D: 0.2188 Loss_G: 0.0701 D(x): 0.9137 D(G(z)): 0.2044: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[69/150] Loss_D: 0.5866 Loss_G: 0.0685 D(x): 0.7514 D(G(z)): 0.2404: 100%|██████████| 7/7 [00:18<00:00,  2.62s/it]\n",
            "[70/150] Loss_D: 0.3490 Loss_G: 0.0689 D(x): 0.7725 D(G(z)): 0.0736: 100%|██████████| 7/7 [00:18<00:00,  2.64s/it]\n",
            "[71/150] Loss_D: 0.2281 Loss_G: 0.0684 D(x): 0.7750 D(G(z)): 0.0003: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[72/150] Loss_D: 0.1553 Loss_G: 0.0678 D(x): 0.8502 D(G(z)): 0.0122: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[73/150] Loss_D: 0.1849 Loss_G: 0.0691 D(x): 0.8605 D(G(z)): 0.0296: 100%|██████████| 7/7 [00:18<00:00,  2.64s/it]\n",
            "[74/150] Loss_D: 0.1235 Loss_G: 0.0699 D(x): 0.8868 D(G(z)): 0.0065: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[75/150] Loss_D: 0.1037 Loss_G: 0.0682 D(x): 0.8965 D(G(z)): 0.0004: 100%|██████████| 7/7 [00:18<00:00,  2.62s/it]\n",
            "[76/150] Loss_D: 0.1036 Loss_G: 0.0682 D(x): 0.8997 D(G(z)): 0.0006: 100%|██████████| 7/7 [00:18<00:00,  2.62s/it]\n",
            "[77/150] Loss_D: 0.1015 Loss_G: 0.0673 D(x): 0.8986 D(G(z)): 0.0001: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[78/150] Loss_D: 0.1005 Loss_G: 0.0679 D(x): 0.8997 D(G(z)): 0.0002: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[79/150] Loss_D: 0.1034 Loss_G: 0.0684 D(x): 0.9003 D(G(z)): 0.0002: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[80/150] Loss_D: 0.1154 Loss_G: 0.0677 D(x): 0.8992 D(G(z)): 0.0078: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[81/150] Loss_D: 0.1246 Loss_G: 0.0680 D(x): 0.8780 D(G(z)): 0.0009: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[82/150] Loss_D: 0.1072 Loss_G: 0.0670 D(x): 0.8928 D(G(z)): 0.0000: 100%|██████████| 7/7 [00:18<00:00,  2.64s/it]\n",
            "[83/150] Loss_D: 0.1111 Loss_G: 0.0667 D(x): 0.8893 D(G(z)): 0.0001: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[84/150] Loss_D: 0.1020 Loss_G: 0.0663 D(x): 0.8982 D(G(z)): 0.0052: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[85/150] Loss_D: 0.2324 Loss_G: 0.0673 D(x): 0.8989 D(G(z)): 0.0366: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[86/150] Loss_D: 0.2013 Loss_G: 0.0667 D(x): 0.8220 D(G(z)): 0.0126: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[87/150] Loss_D: 0.2401 Loss_G: 0.0679 D(x): 0.7798 D(G(z)): 0.0199: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[88/150] Loss_D: 0.2049 Loss_G: 0.0674 D(x): 0.7977 D(G(z)): 0.0001: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[89/150] Loss_D: 0.1160 Loss_G: 0.0684 D(x): 0.8844 D(G(z)): 0.0018: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[90/150] Loss_D: 0.1092 Loss_G: 0.0679 D(x): 0.9083 D(G(z)): 0.0150: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[91/150] Loss_D: 0.1107 Loss_G: 0.0685 D(x): 0.9033 D(G(z)): 0.0087: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[92/150] Loss_D: 0.1319 Loss_G: 0.0679 D(x): 0.8721 D(G(z)): 0.0000: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[93/150] Loss_D: 0.1243 Loss_G: 0.0659 D(x): 0.8924 D(G(z)): 0.0101: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[94/150] Loss_D: 0.1537 Loss_G: 0.0659 D(x): 0.8564 D(G(z)): 0.0118: 100%|██████████| 7/7 [00:18<00:00,  2.64s/it]\n",
            "[95/150] Loss_D: 0.1500 Loss_G: 0.0679 D(x): 0.8801 D(G(z)): 0.0302: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[96/150] Loss_D: 0.1736 Loss_G: 0.0672 D(x): 0.8816 D(G(z)): 0.0560: 100%|██████████| 7/7 [00:18<00:00,  2.64s/it]\n",
            "[97/150] Loss_D: 0.1712 Loss_G: 0.0663 D(x): 0.8893 D(G(z)): 0.0563: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[98/150] Loss_D: 0.1382 Loss_G: 0.0667 D(x): 0.8920 D(G(z)): 0.0302: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[99/150] Loss_D: 0.1244 Loss_G: 0.0666 D(x): 0.9057 D(G(z)): 0.0302: 100%|██████████| 7/7 [00:18<00:00,  2.64s/it]\n",
            "[100/150] Loss_D: 0.1237 Loss_G: 0.0680 D(x): 0.9089 D(G(z)): 0.0319: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[101/150] Loss_D: 0.1203 Loss_G: 0.0676 D(x): 0.9097 D(G(z)): 0.0299: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[102/150] Loss_D: 0.1206 Loss_G: 0.0669 D(x): 0.9096 D(G(z)): 0.0302: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[103/150] Loss_D: 0.1204 Loss_G: 0.0668 D(x): 0.9096 D(G(z)): 0.0299: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[104/150] Loss_D: 0.1205 Loss_G: 0.0655 D(x): 0.9096 D(G(z)): 0.0300: 100%|██████████| 7/7 [00:18<00:00,  2.64s/it]\n",
            "[105/150] Loss_D: 0.1204 Loss_G: 0.0653 D(x): 0.9097 D(G(z)): 0.0301: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[106/150] Loss_D: 0.1205 Loss_G: 0.0655 D(x): 0.9097 D(G(z)): 0.0301: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[107/150] Loss_D: 0.1205 Loss_G: 0.0661 D(x): 0.9096 D(G(z)): 0.0301: 100%|██████████| 7/7 [00:18<00:00,  2.64s/it]\n",
            "[108/150] Loss_D: 0.1204 Loss_G: 0.0637 D(x): 0.9098 D(G(z)): 0.0302: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[109/150] Loss_D: 0.1204 Loss_G: 0.0643 D(x): 0.9098 D(G(z)): 0.0302: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[110/150] Loss_D: 0.1204 Loss_G: 0.0650 D(x): 0.9098 D(G(z)): 0.0302: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[111/150] Loss_D: 0.1193 Loss_G: 0.0657 D(x): 0.9098 D(G(z)): 0.0269: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[112/150] Loss_D: 0.1205 Loss_G: 0.0650 D(x): 0.9096 D(G(z)): 0.0300: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[113/150] Loss_D: 0.1219 Loss_G: 0.0649 D(x): 0.9081 D(G(z)): 0.0300: 100%|██████████| 7/7 [00:18<00:00,  2.64s/it]\n",
            "[114/150] Loss_D: 0.1189 Loss_G: 0.0658 D(x): 0.9087 D(G(z)): 0.0198: 100%|██████████| 7/7 [00:18<00:00,  2.64s/it]\n",
            "[115/150] Loss_D: 0.1601 Loss_G: 0.0653 D(x): 0.8667 D(G(z)): 0.0291: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[116/150] Loss_D: 0.1212 Loss_G: 0.0658 D(x): 0.9090 D(G(z)): 0.0302: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[117/150] Loss_D: 0.1205 Loss_G: 0.0648 D(x): 0.9097 D(G(z)): 0.0302: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[118/150] Loss_D: 0.1218 Loss_G: 0.0644 D(x): 0.9086 D(G(z)): 0.0305: 100%|██████████| 7/7 [00:18<00:00,  2.64s/it]\n",
            "[119/150] Loss_D: 0.1223 Loss_G: 0.0647 D(x): 0.9087 D(G(z)): 0.0309: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[120/150] Loss_D: 0.1215 Loss_G: 0.0647 D(x): 0.9097 D(G(z)): 0.0310: 100%|██████████| 7/7 [00:18<00:00,  2.64s/it]\n",
            "[121/150] Loss_D: 0.1206 Loss_G: 0.0645 D(x): 0.9099 D(G(z)): 0.0303: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[122/150] Loss_D: 0.1209 Loss_G: 0.0642 D(x): 0.9099 D(G(z)): 0.0303: 100%|██████████| 7/7 [00:18<00:00,  2.64s/it]\n",
            "[123/150] Loss_D: 0.1211 Loss_G: 0.0633 D(x): 0.9092 D(G(z)): 0.0303: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[124/150] Loss_D: 0.1201 Loss_G: 0.0638 D(x): 0.9099 D(G(z)): 0.0301: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[125/150] Loss_D: 0.1203 Loss_G: 0.0637 D(x): 0.9098 D(G(z)): 0.0301: 100%|██████████| 7/7 [00:18<00:00,  2.62s/it]\n",
            "[126/150] Loss_D: 0.1202 Loss_G: 0.0644 D(x): 0.9099 D(G(z)): 0.0301: 100%|██████████| 7/7 [00:18<00:00,  2.64s/it]\n",
            "[127/150] Loss_D: 0.1200 Loss_G: 0.0646 D(x): 0.9099 D(G(z)): 0.0298: 100%|██████████| 7/7 [00:18<00:00,  2.64s/it]\n",
            "[128/150] Loss_D: 0.1202 Loss_G: 0.0665 D(x): 0.9099 D(G(z)): 0.0300: 100%|██████████| 7/7 [00:18<00:00,  2.64s/it]\n",
            "[129/150] Loss_D: 0.1201 Loss_G: 0.0640 D(x): 0.9099 D(G(z)): 0.0300: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[130/150] Loss_D: 0.1201 Loss_G: 0.0643 D(x): 0.9099 D(G(z)): 0.0301: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[131/150] Loss_D: 0.1198 Loss_G: 0.0637 D(x): 0.9099 D(G(z)): 0.0294: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[132/150] Loss_D: 0.1200 Loss_G: 0.0645 D(x): 0.9100 D(G(z)): 0.0298: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[133/150] Loss_D: 0.1068 Loss_G: 0.0634 D(x): 0.9100 D(G(z)): 0.0006: 100%|██████████| 7/7 [00:18<00:00,  2.64s/it]\n",
            "[134/150] Loss_D: 0.1976 Loss_G: 0.0647 D(x): 0.8044 D(G(z)): 0.0008: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[135/150] Loss_D: 0.1342 Loss_G: 0.0639 D(x): 0.8659 D(G(z)): 0.0000: 100%|██████████| 7/7 [00:18<00:00,  2.65s/it]\n",
            "[136/150] Loss_D: 0.1200 Loss_G: 0.0631 D(x): 0.8898 D(G(z)): 0.0102: 100%|██████████| 7/7 [00:18<00:00,  2.64s/it]\n",
            "[137/150] Loss_D: 0.1314 Loss_G: 0.0629 D(x): 0.8897 D(G(z)): 0.0210: 100%|██████████| 7/7 [00:18<00:00,  2.64s/it]\n",
            "[138/150] Loss_D: 0.1249 Loss_G: 0.0626 D(x): 0.8884 D(G(z)): 0.0076: 100%|██████████| 7/7 [00:18<00:00,  2.64s/it]\n",
            "[139/150] Loss_D: 0.1122 Loss_G: 0.0634 D(x): 0.8878 D(G(z)): 0.0000: 100%|██████████| 7/7 [00:18<00:00,  2.64s/it]\n",
            "[140/150] Loss_D: 0.1113 Loss_G: 0.0636 D(x): 0.8888 D(G(z)): 0.0000: 100%|██████████| 7/7 [00:18<00:00,  2.64s/it]\n",
            "[141/150] Loss_D: 0.1107 Loss_G: 0.0634 D(x): 0.8893 D(G(z)): 0.0000: 100%|██████████| 7/7 [00:18<00:00,  2.65s/it]\n",
            "[142/150] Loss_D: 0.1003 Loss_G: 0.0637 D(x): 0.8997 D(G(z)): 0.0000: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[143/150] Loss_D: 0.1004 Loss_G: 0.0631 D(x): 0.8997 D(G(z)): 0.0001: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[144/150] Loss_D: 0.1004 Loss_G: 0.0627 D(x): 0.8998 D(G(z)): 0.0001: 100%|██████████| 7/7 [00:18<00:00,  2.64s/it]\n",
            "[145/150] Loss_D: 0.0998 Loss_G: 0.0625 D(x): 0.9002 D(G(z)): 0.0001: 100%|██████████| 7/7 [00:18<00:00,  2.64s/it]\n",
            "[146/150] Loss_D: 0.0922 Loss_G: 0.0625 D(x): 0.9097 D(G(z)): 0.0007: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[147/150] Loss_D: 0.0992 Loss_G: 0.0620 D(x): 0.9008 D(G(z)): 0.0000: 100%|██████████| 7/7 [00:18<00:00,  2.66s/it]\n",
            "[148/150] Loss_D: 0.0939 Loss_G: 0.0631 D(x): 0.9064 D(G(z)): 0.0002: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n",
            "[149/150] Loss_D: 0.0885 Loss_G: 0.0630 D(x): 0.9176 D(G(z)): 0.0018: 100%|██████████| 7/7 [00:18<00:00,  2.63s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zGJyVq9Qwzr1"
      },
      "source": [
        ""
      ],
      "execution_count": 9,
      "outputs": []
    }
  ]
}